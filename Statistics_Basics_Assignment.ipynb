{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Statistics Basics Assignment"
      ],
      "metadata": {
        "id": "_RzwGaq-dnh7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Theory Questions"
      ],
      "metadata": {
        "id": "rbFANQd3dnj2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. What is statistics, and why is it important ?\n",
        "\n",
        "- Statistics is the branch of mathematics that deals with collecting, analyzing, interpreting, presenting, and organizing data. It helps us make sense of complex data by summarizing patterns, identifying trends, and drawing conclusions.\n",
        "\n"
      ],
      "metadata": {
        "id": "uxpiku-cdnog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. What are the two main types of statistics ?\n",
        "\n",
        "- The two main types of statistics are:\n",
        "  - Descriptive statistics\n",
        "  - Infrential statistics"
      ],
      "metadata": {
        "id": "v8lgGCg4dnqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. What are descriptive statistics ?\n",
        "- Descriptive statistics are methods used to summarize, organize, and present data in an informative way. They help you quickly understand the basic features and patterns in a dataset without drawing conclusions beyond the data itself."
      ],
      "metadata": {
        "id": "4A49zkY1dnvT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. What is inferential statistics ?\n",
        "- Inferential statistics is the branch of statistics that allows you to make predictions, decisions, or generalizations about a larger population based on a sample of data. It helps you go beyond just describing data (as in descriptive statistics) and make conclusions that extend beyond the immediate data.\n",
        "\n"
      ],
      "metadata": {
        "id": "U0d4MDwldnw4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. What is sampling in statistics ?\n",
        "- Sampling in statistics is the process of selecting a subset (sample) from a larger group (population) to collect data and make inferences about the entire population. Since studying an entire population is often expensive, time-consuming, or impossible, sampling provides a practical and efficient way to gather information."
      ],
      "metadata": {
        "id": "NKnYPtEIdn2K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6. What are the different types of sampling methods ?\n",
        "- Sampling methods are broadly classified into two categories:\n",
        "- Probablity Sampling\n",
        "  - Simple Random Sampling\n",
        "  - Systematic Sampling\n",
        "  - Stratified Sampling\n",
        "  - Cluster Sampling\n",
        "\n",
        "- Non-Probability Sampling\n",
        "  - Convinience Sampling\n",
        "  - Judgemental Sampling\n",
        "  - Quota Sampling\n",
        "  - Snowball Sampling"
      ],
      "metadata": {
        "id": "GlPbJteZdn4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7. What is the difference between random and non-random sampling ?\n",
        "The difference between random and non-random sampling lies in how the participants or data points are selected from the population, and this directly affects the accuracy, bias, and generalizability of the results.\n",
        "\n",
        "- Random Sampling (Probability Sampling)\n",
        "\n",
        "- Key Features:\n",
        "  - Selection is based on chance, often using tools like random number generators.\n",
        "\n",
        "  - Reduces selection bias.\n",
        "\n",
        "  - Results can be generalized to the entire population.\n",
        "\n",
        "- Non-Random Sampling\n",
        "\n",
        "- Key Features:\n",
        "  - Easier and quicker to carry out\n",
        "\n",
        "  - Prone to selection bias\n",
        "\n",
        "  - Results are less generalizable"
      ],
      "metadata": {
        "id": "pHYCnRHDdn9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###8. Define and give examples of qualitative and quantitative data.\n",
        "- Qualitative Data (Categorical Data):-  \n",
        "Qualitative data describes qualities or characteristics that cannot be measured with numbers. Instead, it is categorized based on traits or labels.\n",
        "\n",
        "- Examples-\n",
        "  - Colors: Red, blue, green\n",
        "\n",
        "  - Gender: Male, female, non-binary\n",
        "\n",
        "  - Marital status: Single, married, divorced\n",
        "\n",
        "  - Type of car: Sedan, SUV, truck\n",
        "\n",
        "  - Feedback: \"Satisfied,\" \"Neutral,\" \"Unsatisfied\"\n",
        "\n",
        "\n",
        "- Quantitative Data (Numerical Data):-  \n",
        "Quantitative data is numerical and can be measured or counted. It represents quantities, amounts, or values.\n",
        "\n",
        "- Examples-\n",
        "  - Age: 21, 35, 42\n",
        "\n",
        "  - Height: 160 cm, 180 cm\n",
        "\n",
        "  - Income: $3,000, $5,500\n",
        "\n",
        "  - Temperature: 22¬∞C, 37¬∞C\n",
        "\n",
        "  - Number of children: 0, 2, 4"
      ],
      "metadata": {
        "id": "rHs_6zO_dn-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###9. What are the different types of data in statistics ?\n",
        "- Main Types of Data in Statistics:\n",
        "\n",
        "- Qualitative (Categorical) Data-\n",
        "  - Nominal Data\n",
        "  - Ordinal Data\n",
        "\n",
        "- Quantitative (Numerical) Data-\n",
        "  - Discrete Data\n",
        "  - Continous Data"
      ],
      "metadata": {
        "id": "J2vAHNgYdoLj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###10. Explain nominal, ordinal, interval, and ratio levels of measurement.\n",
        "\n",
        "- In statistics, levels of measurement (also called scales of measurement) describe how data values are categorized, ordered, and measured. There are four levels: nominal, ordinal, interval, and ratio ‚Äî each providing a different amount of information and determining which statistical methods can be applied.\n",
        "\n",
        "- Nominal Level (Categorical - No Order):-  \n",
        "Data is categorized into distinct groups or labels with no inherent order or ranking.\n",
        "\n",
        "- Ordinal Level (Categorical - Ordered):-  \n",
        "Data is grouped into ordered categories, but the intervals between ranks are not equal or known.\n",
        "\n",
        "- Interval Level (Numerical - Equal Intervals, No True Zero):-  \n",
        "Numerical data with equal distances between values, but no true zero point (zero does not mean ‚Äúnone‚Äù).\n",
        "\n",
        "- Ratio Level (Numerical - Equal Intervals, True Zero):-  \n",
        "Like interval data, but with a true zero point, allowing for all mathematical operations, including ratios."
      ],
      "metadata": {
        "id": "GxsbyfT9doNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###11. What is the measure of central tendency ?\n",
        "Measures of central tendency are statistical measures used to describe the center or typical value of a dataset. They summarize a set of data by identifying the single value that best represents the data. The three most common measures of central tendency are:\n",
        "\n",
        "  - Mean\n",
        "  - Median\n",
        "  - Mode"
      ],
      "metadata": {
        "id": "_ikXvipydoSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###12. Define mean, median, and mode.\n",
        "- Mean (Arithmetic Average):-  \n",
        "The mean is the average of all the numbers in a dataset. It is calculated by adding up all the data points and dividing by the number of points.\n",
        "\n",
        "- Median:-  \n",
        "The median is the middle value when the data is ordered from least to greatest. If the number of values is even, the median is the average of the two middle numbers.\n",
        "\n",
        "- Mode:-  \n",
        "The mode is the value that appears most frequently in a dataset."
      ],
      "metadata": {
        "id": "GJmQwaQJdoUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###13. What is the significance of the measure of central tendency ?\n",
        "- The measures of central tendency‚Äîmean, median, and mode‚Äîare crucial in statistics because they provide a single value that represents the center or typical value of a dataset. Understanding and using these measures helps summarize large sets of data, enabling better insights and decision-making.\n",
        "\n",
        "- The measures of central tendency are foundational to understanding data. They allow us to simplify and summarize data, detect patterns, make comparisons, and make informed decisions. By choosing the right measure (mean, median, or mode), you can get the most accurate insight into your dataset, whether it‚Äôs skewed, has outliers, or is symmetrical."
      ],
      "metadata": {
        "id": "HwvsCy5EdoZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###14. What is variance, and how is it calculated ?\n",
        "- Variance is a statistical measure that represents the spread or dispersion of a set of data points. It tells us how much the individual data points differ from the mean (average) of the dataset. A higher variance means the data points are spread out more, while a lower variance means they are closer to the mean.\n"
      ],
      "metadata": {
        "id": "1ZaulEAkdobP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###15. What is standard deviation, and why is it important ?\n",
        "- Standard Deviation is a measure of dispersion or spread in a dataset. It tells you how much individual data points in a dataset deviate from the mean (average). In simpler terms, standard deviation shows how spread out the values are around the mean‚Äîwhether they are closely packed together or scattered widely.\n",
        "- Standard Deviation is important for:\n",
        "  - Understanding Data Variability\n",
        "\n",
        "  - Real-World Applications\n",
        "\n",
        "  - Comparison Between Datasets\n",
        "\n",
        "  - Further Statistical Analysis"
      ],
      "metadata": {
        "id": "7pwJiRwidohW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###16. Define and explain the term range in statistics.\n",
        "\n",
        "- Range in statistics is a measure of dispersion that represents the difference between the maximum and minimum values in a dataset. It gives a quick sense of how spread out the values are, indicating the extent of variation between the highest and lowest data points.\n",
        "\n",
        "Formula for Range:\n",
        "The range is calculated using the following formula:\n",
        "\n",
        "Range\n",
        "=\n",
        "Maximum¬†Value\n",
        "‚àí\n",
        "Minimum¬†Value\n",
        "Range=Maximum¬†Value‚àíMinimum¬†Value\n",
        "Where:\n",
        "\n",
        "Maximum Value is the largest number in the dataset.\n",
        "\n",
        "Minimum Value is the smallest number in the dataset.\n",
        "\n",
        "Example:\n",
        "Let's say you have the following dataset of test scores: 12, 15, 18, 22, 29, 34\n",
        "\n",
        "Find the Maximum Value: The largest value is 34.\n",
        "\n",
        "Find the Minimum Value: The smallest value is 12.\n",
        "\n",
        "Now, calculate the range:\n",
        "\n",
        "Range\n",
        "=\n",
        "34\n",
        "‚àí\n",
        "12\n",
        "=\n",
        "22\n",
        "Range=34‚àí12=22\n",
        "So, the range of this dataset is 22.\n",
        "\n",
        "Significance of Range:\n",
        "Measure of Spread:\n",
        "\n",
        "The range provides a simple, quick way to measure the spread or variability in a dataset.\n",
        "\n",
        "A larger range suggests that the data points are spread out over a wider range of values, while a smaller range indicates that the values are clustered more closely together.\n",
        "\n",
        "Easy to Compute:\n",
        "\n",
        "The range is easy to calculate, as it only requires identifying the minimum and maximum values in the dataset.\n",
        "\n",
        "Limitations:\n",
        "\n",
        "The range does not account for the distribution of data points within the range. For example, it can be influenced heavily by outliers or extreme values. If a dataset has one very large or very small number, it can dramatically affect the range, even if most of the data points are clustered together.\n",
        "\n",
        "Because of this, the range is often not sufficient on its own for describing the spread of data, and is usually complemented by other measures such as variance and standard deviation.\n",
        "\n",
        "Example with Extreme Values (Outliers):\n",
        "Consider the dataset: 1, 2, 3, 4, 100\n",
        "\n",
        "Maximum Value: 100\n",
        "\n",
        "Minimum Value: 1\n",
        "\n",
        "Range\n",
        "=\n",
        "100\n",
        "‚àí\n",
        "1\n",
        "=\n",
        "99\n",
        "Range=100‚àí1=99\n",
        "Even though most of the data points (1, 2, 3, 4) are clustered close together, the presence of the extreme value 100 increases the range to 99, which may not accurately represent the spread of most of the data points. This highlights one of the limitations of using range alone."
      ],
      "metadata": {
        "id": "e8CZV98adojA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###17. What is the difference between variance and standard deviation ?\n",
        "- Variance and standard deviation are both measures of data spread, but they differ in how they are calculated and interpreted. Variance is the average of the squared differences from the mean, while standard deviation is the square root of the variance. In essence, variance provides a measure of the spread in squared units, while standard deviation provides a measure of spread in the original units of the data."
      ],
      "metadata": {
        "id": "qMwIIv15dooz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###18. What is skewness in a dataset ?\n",
        "- Skewness in a dataset refers to the measure of the asymmetry or distortion of the data distribution. It tells you about the direction in which the data is lopsided relative to the mean.\n",
        "- In simpler terms, skewness indicates whether the data is concentrated more on the left side (negative skew) or on the right side (positive skew) of the mean."
      ],
      "metadata": {
        "id": "22KupSyxdoqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###19. What does it mean if a dataset is positively or negatively skewed ?\n",
        "When a dataset is described as positively skewed or negatively skewed, it refers to the direction of the tail in its distribution and how the data points are distributed around the central tendency (mean, median).\n",
        "-  Positive Skew (Right Skew):  \n",
        "A dataset is positively skewed if it has a long right tail, meaning the right side of the distribution is more spread out with relatively few larger values (outliers) pulling the distribution in that direction.\n",
        "\n",
        "What it means:\n",
        "\n",
        "  - The mean is greater than the median, and the mode is typically the smallest.\n",
        "\n",
        "  - Most of the data points are concentrated on the left side of the distribution, with a few unusually large values stretching the tail to the right.\n",
        "\n",
        "- Negative Skew (Left Skew):  \n",
        "A dataset is negatively skewed if it has a long left tail, meaning the left side of the distribution is more spread out with relatively few smaller values (outliers) pulling the distribution in that direction.\n",
        "\n",
        "What it means:\n",
        "\n",
        "  - The mean is less than the median, and the mode is typically the largest.\n",
        "\n",
        "  - Most of the data points are concentrated on the right side of the distribution, with a few unusually small values stretching the tail to the left."
      ],
      "metadata": {
        "id": "ZxYql2yndowG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###20. Define and explain kurtosis.\n",
        "\n",
        "- Kurtosis is a statistical measure used to describe the shape of a data distribution, particularly the height and sharpness of its peak compared to a normal distribution. Specifically, it measures how heavy or light the tails of a distribution are and how concentrated the data is around the mean.\n",
        "\n",
        "- In simpler terms, kurtosis tells us whether the data have more outliers than a normal distribution or fewer, by focusing on the tails of the distribution and the peak around the mean.\n",
        "\n",
        "Types of Kurtosis:\n",
        "\n",
        "- Leptokurtic (High Kurtosis):  \n",
        "A leptokurtic distribution has a taller, sharper peak at the mean and heavier tails compared to a normal distribution.\n",
        "This means that there are more extreme outliers (values that fall far from the mean) than in a normal distribution.\n",
        "\n",
        "  - Characteristics: High kurtosis implies that the distribution has more data points in the extremes, resulting in higher chances of extreme values or outliers.\n",
        "\n",
        "  - Example: Stock market returns during periods of high volatility can exhibit leptokurtic distributions, with more extreme highs and lows than a normal distribution.\n",
        "\n",
        "- Platykurtic (Low Kurtosis):  \n",
        "A platykurtic distribution has a flatter peak and lighter tails compared to a normal distribution.\n",
        "This means that the distribution has fewer extreme outliers or unusually large or small values.\n",
        "\n",
        "  - Characteristics: Low kurtosis suggests that the data is more evenly distributed, with fewer extreme events.\n",
        "\n",
        "  - Example: A dataset of scores from a standardized test where most people score in the average range could be platykurtic.\n",
        "\n",
        "- Mesokurtic (Normal Kurtosis):  \n",
        "A mesokurtic distribution has a normal bell-shaped curve, with a moderate peak and normal tail behavior, similar to the standard normal distribution (a Gaussian distribution).\n",
        "\n",
        "  - Characteristics: A mesokurtic distribution does not have unusually heavy or light tails. It represents a typical distribution, like the normal distribution.\n",
        "\n",
        "  - Example: Heights of adult humans, where most people are close to the average height, could follow a mesokurtic distribution."
      ],
      "metadata": {
        "id": "ugCvzCsYdox2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###21. What is the purpose of covariance ?\n",
        "- Covariance is a statistical measure used to determine the relationship between two variables and how they change together. In simple terms, it indicates whether two variables tend to increase or decrease at the same time, or if one variable increases while the other decreases.\n"
      ],
      "metadata": {
        "id": "uAJQko2Fdo3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###22. What does correlation measure in statistics ?\n",
        "- Correlation is a statistical measure that describes the strength and direction of the relationship between two variables. It quantifies how two variables move together, and whether they increase or decrease at the same time.\n",
        "\n",
        "- In simpler terms, correlation tells you how closely two variables are related to each other. It is used to understand whether an increase in one variable tends to be associated with an increase or decrease in another variable."
      ],
      "metadata": {
        "id": "j_FyoPs_do6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###23. What is the difference between covariance and correlation ?\n",
        "- Covariance and correlation are both statistical measures that describe the relationship between two variables, but they differ in how they quantify that relationship. Covariance measures the direction and magnitude of the linear relationship, while correlation measures the strength and direction of the linear relationship, but in a standardized way. Covariance can take any value from negative to positive infinity, and its value can be affected by the scale of the variables. Correlation, on the other hand, is standardized and always falls between -1 and 1, making it easier to interpret the strength of the relationship, according to Statistics and the University of Michigan and SlideShare.\n",
        "\n"
      ],
      "metadata": {
        "id": "_oD9T7qIdo-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###24. What are some real-world applications of statistics?\n",
        "- Statistics finds widespread applications in various real-world scenarios, from business decision-making and scientific research to healthcare analysis and quality control. It's used in fields like finance, education, and government to understand trends, evaluate policies, and make informed decisions.\n",
        "- Here are some specific examples:\n",
        "  - Business: Analyzing market trends, assessing sales results, forecasting demand, and managing risk.\n",
        "  - Healthcare: Studying disease prevalence, evaluating treatment effectiveness, and conducting clinical trials.\n",
        "  - Government: Analyzing demographic changes, evaluating public policy effectiveness, and allocating resources.\n",
        "  - Education: Determining teaching method effectiveness, assessing student performance, and evaluating educational programs.\n",
        "  - Finance: Analyzing financial markets, predicting market trends, and managing investment portfolios.\n",
        "  - Sports: Analyzing player performance, evaluating team capabilities, and predicting game outcomes.\n",
        "  - Weather Forecasting: Predicting weather patterns and understanding climate change.\n",
        "  - Quality Control: Monitoring production processes, ensuring product quality, and reducing defects.\n",
        "  - Market Research: Understanding consumer preferences, identifying target markets, and developing marketing strategies.\n",
        "  - Scientific Research: Conducting experiments, analyzing data, and drawing conclusions.\n",
        "  - Social Sciences: Studying social trends, analyzing public opinion, and conducting surveys.\n"
      ],
      "metadata": {
        "id": "wxFA_eg0dpBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Practical Questions"
      ],
      "metadata": {
        "id": "J4tfqWb5qlDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#How do you calculate the mean, median, and mode of a dataset ?\n",
        "\n",
        "'''\n",
        "#### **How to calculate the mean:**\n",
        "\n",
        "1. **Add all the values together** in the dataset.\n",
        "2. **Divide** the sum by the **number of values** in the dataset.\n",
        "\n",
        "#### **Formula**:\n",
        "\n",
        "$$\n",
        "\\text{Mean} = \\frac{\\sum X_i}{n}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $X_i$ represents each value in the dataset.\n",
        "* $n$ is the number of values in the dataset.\n",
        "\n",
        "#### **Example**:\n",
        "\n",
        "For the dataset: **5, 8, 10, 12, 15**\n",
        "\n",
        "1. Add all the numbers: $5 + 8 + 10 + 12 + 15 = 50$\n",
        "2. Divide by the number of values: $50 \\div 5 = 10$\n",
        "\n",
        "So, the **mean** is **10**.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Median**\n",
        "\n",
        "The **median** is the middle value in a dataset when the numbers are arranged in **ascending** or **descending** order. If the dataset has an **odd number** of values, the median is the middle value. If the dataset has an **even number** of values, the median is the **average** of the two middle values.\n",
        "\n",
        "#### **How to calculate the median:**\n",
        "\n",
        "1. **Sort the dataset** in ascending (or descending) order.\n",
        "2. If the number of values is **odd**, the median is the value in the middle.\n",
        "3. If the number of values is **even**, the median is the average of the two middle values.\n",
        "\n",
        "#### **Example 1 (Odd number of values)**:\n",
        "\n",
        "For the dataset: **5, 8, 10, 12, 15**\n",
        "\n",
        "1. Sort the numbers (already sorted): **5, 8, 10, 12, 15**\n",
        "2. The middle value is **10**, so the median is **10**.\n",
        "\n",
        "#### **Example 2 (Even number of values)**:\n",
        "\n",
        "For the dataset: **5, 8, 10, 12**\n",
        "\n",
        "1. Sort the numbers (already sorted): **5, 8, 10, 12**\n",
        "2. The two middle values are **8** and **10**.\n",
        "3. Find the average: $\\frac{8 + 10}{2} = 9$.\n",
        "\n",
        "So, the **median** is **9**.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Mode**\n",
        "\n",
        "The **mode** is the value that **appears most frequently** in the dataset. A dataset can have:\n",
        "\n",
        "* **One mode** (unimodal),\n",
        "* **Two modes** (bimodal),\n",
        "* **Multiple modes** (multimodal), or\n",
        "* **No mode** if no number repeats.\n",
        "\n",
        "#### **How to calculate the mode:**\n",
        "\n",
        "1. Count how many times each value appears in the dataset.\n",
        "2. The value(s) that appear the most are the **mode(s)**.\n",
        "\n",
        "#### **Example 1 (Single mode)**:\n",
        "\n",
        "For the dataset: **5, 8, 10, 10, 12, 15**\n",
        "\n",
        "* **10** appears twice, and all other numbers appear once.\n",
        "* So, the **mode** is **10**.\n",
        "\n",
        "#### **Example 2 (Multiple modes)**:\n",
        "\n",
        "For the dataset: **5, 8, 8, 10, 12, 12, 15**\n",
        "\n",
        "* Both **8** and **12** appear twice.\n",
        "* So, the dataset is **bimodal**, with modes of **8** and **12**.\n",
        "\n",
        "#### **Example 3 (No mode)**:\n",
        "\n",
        "For the dataset: **5, 8, 10, 12, 15**\n",
        "\n",
        "* Each number appears only once.\n",
        "* So, there is **no mode**.\n",
        "\n",
        "---\n",
        "'''"
      ],
      "metadata": {
        "id": "nXUxTwZnq3Lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.  Write a Python program to compute the variance and standard deviation of a dataset.\n",
        "\n",
        "import math\n",
        "\n",
        "# Function to calculate variance\n",
        "def calculate_variance(data):\n",
        "    n = len(data)\n",
        "    mean = sum(data) / n  # Calculate the mean\n",
        "    squared_diff = [(x - mean) ** 2 for x in data]  # List of squared differences\n",
        "    variance = sum(squared_diff) / n  # Variance formula (Population Variance)\n",
        "    return variance\n",
        "\n",
        "# Function to calculate standard deviation\n",
        "def calculate_standard_deviation(data):\n",
        "    variance = calculate_variance(data)  # Get the variance first\n",
        "    standard_deviation = math.sqrt(variance)  # Standard deviation is the square root of variance\n",
        "    return standard_deviation\n",
        "\n",
        "# Example dataset\n",
        "data = [5, 8, 10, 12, 15]\n",
        "\n",
        "# Calculate and display the variance and standard deviation\n",
        "variance = calculate_variance(data)\n",
        "standard_deviation = calculate_standard_deviation(data)\n",
        "\n",
        "print(f\"Dataset: {data}\")\n",
        "print(f\"Variance: {variance}\")\n",
        "print(f\"Standard Deviation: {standard_deviation}\")\n"
      ],
      "metadata": {
        "id": "rgxBEl4OrSWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Create a dataset and classify it into nominal, ordinal, interval, and ratio types.\n",
        "\n",
        "#Here's an example where we create a simple dataset and classify each variable into nominal, ordinal, interval, and ratio types\n",
        "#based on the level of measurement in statistics.\n",
        "\n",
        "| Name    | Education Level | Temperature (¬∞C) | Age (years) |\n",
        "| ------- | --------------- | ---------------- | ----------- |\n",
        "| Alice   | Bachelor's      | 30               | 25          |\n",
        "| Bob     | Master's        | 25               | 30          |\n",
        "| Charlie | High School     | 28               | 22          |\n",
        "| Diana   | PhD             | 26               | 35          |\n",
        "| Eva     | Bachelor's      | 27               | 28          |\n",
        "\n",
        "\n",
        "# Classification of Variables:\n",
        "| Variable             | Data Type    | Level of Measurement | Explanation                                                                                                                      |\n",
        "| -------------------- | ------------ | -------------------- | -------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **Name**             | Qualitative  | **Nominal**          | Names are used as identifiers without any order. They are just labels.                                                           |\n",
        "| **Education Level**  | Qualitative  | **Ordinal**          | Education levels can be ranked (High School < Bachelor's < Master's < PhD), but the differences between ranks aren‚Äôt measurable. |\n",
        "| **Temperature (¬∞C)** | Quantitative | **Interval**         | Temperature in Celsius has measurable differences, but **no true zero** (0¬∞C doesn't mean \"no temperature\").                     |\n",
        "| **Age (years)**      | Quantitative | **Ratio**            | Age has a true zero point and both differences and ratios are meaningful (e.g., 40 is twice 20).                                 |\n",
        "\n"
      ],
      "metadata": {
        "id": "rS314fw6rSYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Implement sampling techniques like random sampling and stratified sampling.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Step-by-Step Sampling Techniques**\n",
        "\n",
        "First, let‚Äôs create a **sample dataset**:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Sample dataset\n",
        "data = {\n",
        "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eva', 'Frank', 'Grace', 'Henry', 'Ivy', 'Jack'],\n",
        "    'Age': [25, 30, 22, 35, 28, 40, 31, 29, 26, 24],\n",
        "    'Education': ['Bachelor', 'Master', 'High School', 'PhD', 'Bachelor',\n",
        "                  'PhD', 'Master', 'Bachelor', 'High School', 'Master']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(\"Full Dataset:\\n\", df)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üé≤ **1. Random Sampling**\n",
        "\n",
        "**Random sampling** selects items from the population where every item has an **equal chance** of being chosen.\n",
        "\n",
        "```python\n",
        "# Randomly select 3 samples from the dataset\n",
        "random_sample = df.sample(n=3, random_state=1)  # random_state for reproducibility\n",
        "\n",
        "print(\"\\nRandom Sample:\\n\", random_sample)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ **2. Stratified Sampling**\n",
        "\n",
        "**Stratified sampling** divides the population into **strata (groups)** based on a categorical variable (e.g., Education), and then samples proportionally from each group.\n",
        "\n",
        "```python\n",
        "# Stratified sampling based on 'Education'\n",
        "# We'll take 1 sample from each education level\n",
        "stratified_sample = df.groupby('Education', group_keys=False).apply(lambda x: x.sample(n=1, random_state=1))\n",
        "\n",
        "print(\"\\nStratified Sample by Education:\\n\", stratified_sample)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üì¶ **Output Example**\n",
        "\n",
        "Suppose you run the above, you might get:\n",
        "\n",
        "```\n",
        "Full Dataset:\n",
        "      Name  Age    Education\n",
        "0   Alice   25     Bachelor\n",
        "1     Bob   30       Master\n",
        "2 Charlie   22  High School\n",
        "3   Diana   35          PhD\n",
        "4     Eva   28     Bachelor\n",
        "5   Frank   40          PhD\n",
        "6   Grace   31       Master\n",
        "7   Henry   29     Bachelor\n",
        "8     Ivy   26  High School\n",
        "9    Jack   24       Master\n",
        "\n",
        "Random Sample:\n",
        "     Name  Age Education\n",
        "2  Charlie   22 High School\n",
        "9    Jack   24     Master\n",
        "6   Grace   31     Master\n",
        "\n",
        "Stratified Sample by Education:\n",
        "      Name  Age    Education\n",
        "0   Alice   25     Bachelor\n",
        "2 Charlie   22  High School\n",
        "1     Bob   30       Master\n",
        "3   Diana   35          PhD\n",
        "```\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "ePgbNV2CrSdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Write a Python function to calculate the range of a dataset.\n",
        "\n",
        "def calculate_range(data):\n",
        "    if not data:\n",
        "        return None  # Handle empty list\n",
        "    data_range = max(data) - min(data)\n",
        "    return data_range\n",
        "\n",
        "# Example usage\n",
        "dataset = [5, 8, 10, 12, 15]\n",
        "result = calculate_range(dataset)\n",
        "\n",
        "print(f\"Dataset: {dataset}\")\n",
        "print(f\"Range: {result}\")\n"
      ],
      "metadata": {
        "id": "6F6CXML_rSfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Create a dataset and plot its histogram to visualize skewness.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create two skewed datasets\n",
        "np.random.seed(42)  # For reproducibility\n",
        "\n",
        "# Positively skewed (right-skewed): Most values are small\n",
        "positive_skew = np.random.exponential(scale=2.0, size=1000)\n",
        "\n",
        "# Negatively skewed (left-skewed): Invert the exponential\n",
        "negative_skew = -np.random.exponential(scale=2.0, size=1000)\n",
        "\n",
        "# Plotting the histograms\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Plot positively skewed data\n",
        "sns.histplot(positive_skew, kde=True, color='skyblue', ax=axs[0])\n",
        "axs[0].set_title(\"Positively Skewed Data (Right Skew)\")\n",
        "axs[0].set_xlabel(\"Value\")\n",
        "axs[0].set_ylabel(\"Frequency\")\n",
        "\n",
        "# Plot negatively skewed data\n",
        "sns.histplot(negative_skew, kde=True, color='salmon', ax=axs[1])\n",
        "axs[1].set_title(\"Negatively Skewed Data (Left Skew)\")\n",
        "axs[1].set_xlabel(\"Value\")\n",
        "axs[1].set_ylabel(\"Frequency\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "r9WsixjqrSkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Calculate skewness and kurtosis of a dataset using Python libraries.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "# Example dataset\n",
        "data = [5, 8, 10, 12, 15, 18, 20, 21, 25, 100]  # Includes an outlier to show effect on skew/kurtosis\n",
        "\n",
        "# Convert to pandas Series (optional, but useful)\n",
        "series = pd.Series(data)\n",
        "\n",
        "# Calculate skewness and kurtosis using scipy\n",
        "skewness_value = skew(series)\n",
        "kurtosis_value = kurtosis(series)  # By default, this returns excess kurtosis (subtracts 3)\n",
        "\n",
        "# If you want normal (Fisher=False) kurtosis (actual kurtosis), use:\n",
        "kurtosis_normal = kurtosis(series, fisher=False)\n",
        "\n",
        "# Output\n",
        "print(f\"Dataset: {data}\")\n",
        "print(f\"Skewness: {skewness_value:.4f}\")\n",
        "print(f\"Excess Kurtosis (Fisher's definition): {kurtosis_value:.4f}\")\n",
        "print(f\"Standard Kurtosis (Pearson's definition): {kurtosis_normal:.4f}\")\n"
      ],
      "metadata": {
        "id": "cI7CSxphrSmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Generate a dataset and demonstrate positive and negative skewness.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import skew\n",
        "\n",
        "# Seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate datasets\n",
        "# Positively skewed (right-skewed)\n",
        "positive_skew = np.random.exponential(scale=2, size=1000)\n",
        "\n",
        "# Negatively skewed (left-skewed) by inverting the exponential distribution\n",
        "negative_skew = -np.random.exponential(scale=2, size=1000)\n",
        "\n",
        "# Calculate skewness\n",
        "pos_skewness = skew(positive_skew)\n",
        "neg_skewness = skew(negative_skew)\n",
        "\n",
        "# Plot histograms\n",
        "fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Positive skew histogram\n",
        "sns.histplot(positive_skew, kde=True, color='skyblue', ax=axs[0])\n",
        "axs[0].set_title(f\"Positively Skewed\\nSkewness = {pos_skewness:.2f}\")\n",
        "axs[0].set_xlabel(\"Value\")\n",
        "axs[0].set_ylabel(\"Frequency\")\n",
        "\n",
        "# Negative skew histogram\n",
        "sns.histplot(negative_skew, kde=True, color='salmon', ax=axs[1])\n",
        "axs[1].set_title(f\"Negatively Skewed\\nSkewness = {neg_skewness:.2f}\")\n",
        "axs[1].set_xlabel(\"Value\")\n",
        "axs[1].set_ylabel(\"Frequency\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print skewness values\n",
        "print(f\"Skewness (Positive): {pos_skewness:.4f}\")\n",
        "print(f\"Skewness (Negative): {neg_skewness:.4f}\")\n"
      ],
      "metadata": {
        "id": "fjitPP2orSry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Write a Python script to calculate covariance between two datasets.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Example datasets\n",
        "x = [2, 4, 6, 8, 10]\n",
        "y = [1, 3, 5, 7, 9]\n",
        "\n",
        "# Manual calculation of covariance (population version)\n",
        "def calculate_covariance(x, y):\n",
        "    if len(x) != len(y):\n",
        "        raise ValueError(\"Datasets x and y must be of the same length.\")\n",
        "\n",
        "    n = len(x)\n",
        "    mean_x = sum(x) / n\n",
        "    mean_y = sum(y) / n\n",
        "\n",
        "    cov = sum((x[i] - mean_x) * (y[i] - mean_y) for i in range(n)) / n\n",
        "    return cov\n",
        "\n",
        "# Using NumPy (note: by default, numpy.cov uses sample covariance (n-1))\n",
        "cov_matrix = np.cov(x, y, bias=True)  # bias=True gives population covariance\n",
        "cov_np = cov_matrix[0, 1]  # Off-diagonal element is the covariance\n",
        "\n",
        "# Results\n",
        "manual_cov = calculate_covariance(x, y)\n",
        "\n",
        "print(f\"Dataset X: {x}\")\n",
        "print(f\"Dataset Y: {y}\")\n",
        "print(f\"Covariance (Manual): {manual_cov}\")\n",
        "print(f\"Covariance (NumPy): {cov_np}\")\n"
      ],
      "metadata": {
        "id": "iuaVtWAhrSt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10.  Write a Python script to calculate the correlation coefficient between two datasets.\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Example datasets\n",
        "x = [2, 4, 6, 8, 10]\n",
        "y = [1, 3, 5, 7, 9]\n",
        "\n",
        "# Method 1: Manual calculation\n",
        "def correlation_coefficient(x, y):\n",
        "    if len(x) != len(y):\n",
        "        raise ValueError(\"Both datasets must have the same length.\")\n",
        "\n",
        "    n = len(x)\n",
        "    mean_x = sum(x) / n\n",
        "    mean_y = sum(y) / n\n",
        "\n",
        "    # Calculate covariance\n",
        "    cov = sum((x[i] - mean_x) * (y[i] - mean_y) for i in range(n)) / n\n",
        "\n",
        "    # Calculate standard deviations\n",
        "    std_x = (sum((i - mean_x) ** 2 for i in x) / n) ** 0.5\n",
        "    std_y = (sum((i - mean_y) ** 2 for i in y) / n) ** 0.5\n",
        "\n",
        "    # Pearson correlation coefficient\n",
        "    r = cov / (std_x * std_y)\n",
        "    return r\n",
        "\n",
        "# Method 2: Using scipy\n",
        "r_scipy, _ = pearsonr(x, y)\n",
        "\n",
        "# Results\n",
        "manual_r = correlation_coefficient(x, y)\n",
        "\n",
        "print(f\"Dataset X: {x}\")\n",
        "print(f\"Dataset Y: {y}\")\n",
        "print(f\"Correlation Coefficient (Manual): {manual_r}\")\n",
        "print(f\"Correlation Coefficient (scipy.stats.pearsonr): {r_scipy}\")\n"
      ],
      "metadata": {
        "id": "LRBdvcQkrSzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11. Create a scatter plot to visualize the relationship between two variables.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example datasets\n",
        "x = [2, 4, 6, 8, 10]\n",
        "y = [1, 3, 5, 7, 9]\n",
        "\n",
        "# Create scatter plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(x, y, color='blue', edgecolor='black', s=80)\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('Scatter Plot of X vs Y')\n",
        "plt.xlabel('X Variable')\n",
        "plt.ylabel('Y Variable')\n",
        "\n",
        "# Add grid and show plot\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Asv2odk7rS1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12.  Implement and compare simple random sampling and systematic sampling.\n",
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# Create a sample dataset\n",
        "data = {'ID': list(range(1, 101))}  # 100 records (ID 1 to 100)\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# -----------------------------\n",
        "# Simple Random Sampling\n",
        "# -----------------------------\n",
        "def simple_random_sampling(df, sample_size):\n",
        "    return df.sample(n=sample_size, random_state=42)\n",
        "\n",
        "# -----------------------------\n",
        "# Systematic Sampling\n",
        "# -----------------------------\n",
        "def systematic_sampling(df, sample_size):\n",
        "    step = len(df) // sample_size\n",
        "    start = random.randint(0, step - 1)\n",
        "    indices = range(start, len(df), step)\n",
        "    return df.iloc[list(indices)[:sample_size]]\n",
        "\n",
        "# Sample size\n",
        "sample_size = 10\n",
        "\n",
        "# Apply both sampling techniques\n",
        "simple_random_sample = simple_random_sampling(df, sample_size)\n",
        "systematic_sample = systematic_sampling(df, sample_size)\n",
        "\n",
        "# Print samples\n",
        "print(\"üé≤ Simple Random Sample:\")\n",
        "print(simple_random_sample)\n",
        "\n",
        "print(\"\\nüìè Systematic Sample:\")\n",
        "print(systematic_sample)\n"
      ],
      "metadata": {
        "id": "g5ohdDmQuvUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#13. Calculate the mean, median, and mode of grouped data.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Grouped data (Example: Exam scores)\n",
        "data = {\n",
        "    'Class Interval': ['0-10', '10-20', '20-30', '30-40', '40-50'],\n",
        "    'Frequency': [5, 10, 15, 7, 3]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate class midpoints\n",
        "df['Midpoint'] = [(int(interval.split('-')[0]) + int(interval.split('-')[1])) / 2 for interval in df['Class Interval']]\n",
        "\n",
        "# Calculate mean\n",
        "mean = (df['Frequency'] * df['Midpoint']).sum() / df['Frequency'].sum()\n",
        "\n",
        "# Cumulative frequency for median\n",
        "df['Cumulative Frequency'] = df['Frequency'].cumsum()\n",
        "\n",
        "# Median\n",
        "N = df['Frequency'].sum()\n",
        "median_class = df[df['Cumulative Frequency'] >= N / 2].iloc[0]\n",
        "L = int(median_class['Class Interval'].split('-')[0])  # Lower boundary of the median class\n",
        "F = median_class['Cumulative Frequency'] - median_class['Frequency']  # Cumulative frequency before the median class\n",
        "f = median_class['Frequency']  # Frequency of the median class\n",
        "h = int(median_class['Class Interval'].split('-')[1]) - L  # Class width\n",
        "median = L + ((N / 2 - F) / f) * h\n",
        "\n",
        "# Mode (for the modal class)\n",
        "modal_class = df[df['Frequency'] == df['Frequency'].max()].iloc[0]\n",
        "L_mode = int(modal_class['Class Interval'].split('-')[0])  # Lower boundary of the modal class\n",
        "f1 = modal_class['Frequency']  # Frequency of the modal class\n",
        "f0 = df.iloc[df.index.get_loc(modal_class.name) - 1]['Frequency'] if modal_class.name > 0 else 0  # Frequency of the class before the modal class\n",
        "f2 = df.iloc[df.index.get_loc(modal_class.name) + 1]['Frequency'] if modal_class.name < len(df) - 1 else 0  # Frequency of the class after the modal class\n",
        "h_mode = int(modal_class['Class Interval'].split('-')[1]) - L_mode  # Class width\n",
        "mode = L_mode + ((f1 - f0) / (2 * f1 - f0 - f2)) * h_mode\n",
        "\n",
        "# Results\n",
        "print(f\"Mean: {mean:.2f}\")\n",
        "print(f\"Median: {median:.2f}\")\n",
        "print(f\"Mode: {mode:.2f}\")\n"
      ],
      "metadata": {
        "id": "vT2_EmBGuvWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#14. Simulate data using Python and calculate its central tendency and dispersion.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Simulate Data\n",
        "np.random.seed(42)  # For reproducibility\n",
        "data = np.random.normal(loc=50, scale=10, size=1000)  # Normal distribution (mean=50, std=10, n=1000)\n",
        "\n",
        "# Step 2: Calculate Central Tendency\n",
        "mean = np.mean(data)\n",
        "median = np.median(data)\n",
        "mode = stats.mode(data)[0][0]\n",
        "\n",
        "# Step 3: Calculate Dispersion\n",
        "variance = np.var(data)\n",
        "std_deviation = np.std(data)\n",
        "\n",
        "# Step 4: Print the Results\n",
        "print(f\"Mean: {mean:.2f}\")\n",
        "print(f\"Median: {median:.2f}\")\n",
        "print(f\"Mode: {mode:.2f}\")\n",
        "print(f\"Variance: {variance:.2f}\")\n",
        "print(f\"Standard Deviation: {std_deviation:.2f}\")\n",
        "\n",
        "# Step 5: Visualize the data with a histogram and boxplot\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Histogram\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(data, bins=30, color='skyblue', edgecolor='black')\n",
        "plt.title(\"Histogram of Simulated Data\")\n",
        "plt.xlabel(\"Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "# Boxplot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.boxplot(data, vert=False, patch_artist=True, boxprops=dict(facecolor='lightgreen', color='black'))\n",
        "plt.title(\"Boxplot of Simulated Data\")\n",
        "plt.xlabel(\"Value\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RgOYyVC_uvbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#15. Use NumPy or pandas to summarize a dataset‚Äôs descriptive statistics.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a sample dataset (for example, exam scores)\n",
        "data = {\n",
        "    'Exam Scores': [85, 90, 92, 78, 88, 95, 100, 70, 85, 78, 91, 85, 88, 96, 87]\n",
        "}\n",
        "\n",
        "# Convert to a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Step 1: Summarize the data using Pandas' describe() method\n",
        "summary = df.describe()\n",
        "\n",
        "# Display the summary\n",
        "print(\"Descriptive Statistics using Pandas:\")\n",
        "print(summary)\n"
      ],
      "metadata": {
        "id": "NXYpHclauvdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#16. Plot a boxplot to understand the spread and identify outliers.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Simulate some data (for example, exam scores)\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(loc=50, scale=10, size=1000)  # Normal distribution\n",
        "\n",
        "# Step 1: Create a Boxplot using Seaborn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=data, color='skyblue')\n",
        "\n",
        "# Step 2: Add a title and labels\n",
        "plt.title(\"Boxplot of Simulated Data\", fontsize=14)\n",
        "plt.xlabel(\"Scores\", fontsize=12)\n",
        "\n",
        "# Step 3: Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tCSrT8shuvit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#17. Calculate the interquartile range (IQR) of a dataset.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Simulate some data (for example, exam scores)\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(loc=50, scale=10, size=1000)  # Normal distribution\n",
        "\n",
        "# Step 1: Calculate the first and third quartiles (Q1 and Q3)\n",
        "Q1 = np.percentile(data, 25)  # 25th percentile\n",
        "Q3 = np.percentile(data, 75)  # 75th percentile\n",
        "\n",
        "# Step 2: Calculate the Interquartile Range (IQR)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Step 3: Display the results\n",
        "print(f\"First Quartile (Q1): {Q1}\")\n",
        "print(f\"Third Quartile (Q3): {Q3}\")\n",
        "print(f\"Interquartile Range (IQR): {IQR}\")\n"
      ],
      "metadata": {
        "id": "EU4PVMTauvkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#18. Implement Z-score normalization and explain its significance.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Simulate a dataset (for example, exam scores)\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(loc=50, scale=10, size=1000)  # Normal distribution\n",
        "\n",
        "# Step 1: Calculate the mean and standard deviation of the dataset\n",
        "mean = np.mean(data)\n",
        "std_dev = np.std(data)\n",
        "\n",
        "# Step 2: Apply Z-score normalization\n",
        "z_scores = (data - mean) / std_dev\n",
        "\n",
        "# Step 3: Print some results\n",
        "print(f\"Original Data Mean: {mean:.2f}, Standard Deviation: {std_dev:.2f}\")\n",
        "print(f\"Mean of Z-scores: {np.mean(z_scores):.2f}, Standard Deviation of Z-scores: {np.std(z_scores):.2f}\")\n",
        "\n",
        "# Step 4: Visualize the data before and after Z-score normalization\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plotting original data distribution\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(data, bins=30, color='skyblue', edgecolor='black')\n",
        "plt.title(\"Original Data Distribution\")\n",
        "plt.xlabel(\"Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "# Plotting Z-score normalized data distribution\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(z_scores, bins=30, color='lightgreen', edgecolor='black')\n",
        "plt.title(\"Z-score Normalized Data Distribution\")\n",
        "plt.xlabel(\"Z-score\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#Significance of Z-score Normalization:\n",
        "\n",
        "# Standardization transforms the data to a common scale, without distorting differences in the ranges of values.\n",
        "\n",
        "# It is particularly important when the dataset contains features with different units\n",
        "# (e.g., weight in kilograms and height in centimeters).\n",
        "\n",
        "# Many machine learning algorithms (like k-nearest neighbors, SVMs, and linear regression) assume that\n",
        "# the data is normally distributed and will perform better if the features are standardized."
      ],
      "metadata": {
        "id": "YoyY9G5Iuvp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19. Compare two datasets using their standard deviations.\n",
        "\n",
        "- import numpy as np\n",
        "\n",
        "# Create two example datasets\n",
        "dataset_1 = np.random.normal(loc=50, scale=5, size=1000)  # Mean=50, Std Dev=5\n",
        "dataset_2 = np.random.normal(loc=50, scale=15, size=1000)  # Mean=50, Std Dev=15\n",
        "\n",
        "# Calculate the standard deviation for both datasets\n",
        "std_dev_1 = np.std(dataset_1)\n",
        "std_dev_2 = np.std(dataset_2)\n",
        "\n",
        "# Display the results\n",
        "print(f\"Standard Deviation of Dataset 1: {std_dev_1:.2f}\")\n",
        "print(f\"Standard Deviation of Dataset 2: {std_dev_2:.2f}\")\n",
        "\n",
        "# Interpretation of Results\n",
        "if std_dev_1 > std_dev_2:\n",
        "    print(\"Dataset 1 has a larger spread (higher variability) than Dataset 2.\")\n",
        "elif std_dev_1 < std_dev_2:\n",
        "    print(\"Dataset 2 has a larger spread (higher variability) than Dataset 1.\")\n",
        "else:\n",
        "    print(\"Both datasets have the same spread (same variability).\")\n"
      ],
      "metadata": {
        "id": "8nX0FRwbuvsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#20. Write a Python program to visualize covariance using a heatmap.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Create a sample dataset (e.g., exam scores and study hours)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Simulate some data\n",
        "study_hours = np.random.normal(loc=5, scale=2, size=100)  # Study hours\n",
        "exam_scores = np.random.normal(loc=70, scale=10, size=100)  # Exam scores\n",
        "attendance = np.random.normal(loc=80, scale=5, size=100)  # Attendance percentage\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Study Hours': study_hours,\n",
        "    'Exam Scores': exam_scores,\n",
        "    'Attendance': attendance\n",
        "})\n",
        "\n",
        "# Step 2: Calculate the covariance matrix\n",
        "cov_matrix = data.cov()\n",
        "\n",
        "# Step 3: Visualize the covariance matrix using a heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cov_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=1, vmin=-20, vmax=20)\n",
        "\n",
        "# Step 4: Add titles and labels\n",
        "plt.title(\"Covariance Matrix Heatmap\", fontsize=16)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2Mmib-8RuvxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21. Use seaborn to create a correlation matrix for a dataset.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Create a sample dataset (e.g., exam scores, study hours, and attendance)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Simulating some data\n",
        "study_hours = np.random.normal(loc=5, scale=2, size=100)  # Study hours\n",
        "exam_scores = np.random.normal(loc=70, scale=10, size=100)  # Exam scores\n",
        "attendance = np.random.normal(loc=80, scale=5, size=100)  # Attendance percentage\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Study Hours': study_hours,\n",
        "    'Exam Scores': exam_scores,\n",
        "    'Attendance': attendance\n",
        "})\n",
        "\n",
        "# Step 2: Calculate the correlation matrix\n",
        "correlation_matrix = data.corr()\n",
        "\n",
        "# Step 3: Visualize the correlation matrix using a heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=1, vmin=-1, vmax=1)\n",
        "\n",
        "# Step 4: Add title\n",
        "plt.title(\"Correlation Matrix Heatmap\", fontsize=16)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4RA8u5MGuvz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Generate a dataset and implement both variance and standard deviation computations.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Generate a dataset (e.g., exam scores)\n",
        "np.random.seed(42)  # Set random seed for reproducibility\n",
        "data = np.random.normal(loc=50, scale=10, size=100)  # Dataset with mean=50, std=10, and 100 samples\n",
        "\n",
        "# Step 2: Calculate the variance\n",
        "variance = np.var(data)\n",
        "\n",
        "# Step 3: Calculate the standard deviation\n",
        "std_deviation = np.std(data)\n",
        "\n",
        "# Step 4: Output the results\n",
        "print(f\"Generated Data (First 10 values): {data[:10]}\")\n",
        "print(f\"Variance of the dataset: {variance:.2f}\")\n",
        "print(f\"Standard Deviation of the dataset: {std_deviation:.2f}\")\n"
      ],
      "metadata": {
        "id": "qHsFMxk3uv85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Visualize skewness and kurtosis using Python libraries like matplotlib or seaborn.\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create datasets with different skewness and kurtosis\n",
        "data_positive_skew = np.random.chisquare(df=2, size=1000)  # Positive skew\n",
        "data_negative_skew = np.random.beta(a=2, b=5, size=1000)  # Negative skew\n",
        "data_normal = np.random.normal(loc=50, scale=10, size=1000)  # Normal distribution\n",
        "\n",
        "# Calculate skewness and kurtosis\n",
        "skew_pos = skew(data_positive_skew)\n",
        "kurt_pos = kurtosis(data_positive_skew)\n",
        "skew_neg = skew(data_negative_skew)\n",
        "kurt_neg = kurtosis(data_negative_skew)\n",
        "skew_norm = skew(data_normal)\n",
        "kurt_norm = kurtosis(data_normal)\n",
        "\n",
        "# Set up the subplots\n",
        "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# Plot positive skew data\n",
        "sns.histplot(data_positive_skew, bins=30, kde=True, color=\"skyblue\", ax=axs[0])\n",
        "axs[0].set_title(f\"Positive Skew (Skew: {skew_pos:.2f}, Kurtosis: {kurt_pos:.2f})\")\n",
        "axs[0].set_xlabel(\"Value\")\n",
        "axs[0].set_ylabel(\"Frequency\")\n",
        "\n",
        "# Plot negative skew data\n",
        "sns.histplot(data_negative_skew, bins=30, kde=True, color=\"lightcoral\", ax=axs[1])\n",
        "axs[1].set_title(f\"Negative Skew (Skew: {skew_neg:.2f}, Kurtosis: {kurt_neg:.2f})\")\n",
        "axs[1].set_xlabel(\"Value\")\n",
        "axs[1].set_ylabel(\"Frequency\")\n",
        "\n",
        "# Plot normal distribution data\n",
        "sns.histplot(data_normal, bins=30, kde=True, color=\"lightgreen\", ax=axs[2])\n",
        "axs[2].set_title(f\"Normal Distribution (Skew: {skew_norm:.2f}, Kurtosis: {kurt_norm:.2f})\")\n",
        "axs[2].set_xlabel(\"Value\")\n",
        "axs[2].set_ylabel(\"Frequency\")\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aP4YlScguwCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Implement the Pearson and Spearman correlation coefficients for a dataset.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "# Step 1: Generate a sample dataset\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create random data for two variables: x and y\n",
        "x = np.random.normal(loc=50, scale=10, size=100)  # Variable x (e.g., study hours)\n",
        "y = 2 * x + np.random.normal(loc=0, scale=5, size=100)  # Variable y (e.g., exam scores)\n",
        "\n",
        "# Step 2: Calculate Pearson correlation coefficient\n",
        "pearson_corr, _ = pearsonr(x, y)\n",
        "\n",
        "# Step 3: Calculate Spearman correlation coefficient\n",
        "spearman_corr, _ = spearmanr(x, y)\n",
        "\n",
        "# Step 4: Output the correlation results\n",
        "print(f\"Pearson Correlation Coefficient: {pearson_corr:.2f}\")\n",
        "print(f\"Spearman Correlation Coefficient: {spearman_corr:.2f}\")\n",
        "\n",
        "# Step 5: Visualize the relationship with a scatter plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=x, y=y, color='blue')\n",
        "plt.title(f\"Scatter Plot (Pearson: {pearson_corr:.2f}, Spearman: {spearman_corr:.2f})\")\n",
        "plt.xlabel(\"Study Hours (X)\")\n",
        "plt.ylabel(\"Exam Scores (Y)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-tDBUt6UyKeE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}